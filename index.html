<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control</title>

    <meta name="description" content="Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <meta property="og:image" content="https://robot-teaching.github.io/assets/images/teaser.gif">
    <meta property="og:image:type" content="image/gif">
    <meta property="og:image:width" content="600">
    <meta property="og:image:height" content="400">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://robot-teaching.github.io/"/>
    <meta property="og:title" content="Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control" />
    <meta property="og:description" content="Project page for Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control" />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control" />
    <meta name="twitter:description" content="Project page for Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control." />
    <meta name="twitter:image" content="https://robot-teaching.github.io/assets/images/teaser.png" />


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HQJ6G7EKRG"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-HQJ6G7EKRG');
    </script>
    <style>
    #myVideo {
      width: 100%;
      height: auto;
      object-fit: cover; /* This will make the video cover the full area of the tag */
      border-radius: 5px; /* If you want to keep the border-radius */
    }
  </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            Learning to Learn Faster from Human Feedback with Language Model Predictive Control
                        </h1>
                        <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://icml.cc/">ICML 2023</a>
                    </h3> -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                               
                                 Jacky Liang<span class="icon">
                                    <sup><a href="mailto:liangq@google.com"><i class="fa fa-envelope"></i></a></sup>,
                                  </span>Fei Xia<span class="icon">
                                    <sup><a href="mailto:xiafei@google.com"><i class="fa fa-envelope"></i></a></sup>,
                                  </span>Wenhao Yu<span class="icon">
                                    <sup> <a href="mailto:magicmelon@google.com"><i class="fa fa-envelope"></i></a> </sup>,
                                  </span>Andy Zeng<span class="icon">
                                    <sup><a href="mailto:andyzeng@google.com"><i class="fa fa-envelope"></i></a> </sup>
                                  </span> <br>
                                Montserrat Gonzalez Arenas,
                                Maria Attarian,
                                Maria Bauza,
                                Matthew Bennice,
                                Alex Bewley,
                                Adil Dostmohamed,
                                Chuyuan Kelly Fu,
                                Nimrod Gileadi,
                                Marissa Giustina,
                                Keerthana Gopalakrishnan,
                                Leonard Hasenclever,
                                Jan Humplik,
                                Jasmine Hsu,
                                Nikhil Joshi,
                                Ben Jyenis,
                                Chase Kew,
                                Sean Kirmani,
                                Tsang-Wei Edward Lee,
                                Kuang-Huei Lee,
                                Assaf Hurwitz Michaely,
                                Joss Moore,
                                Ken Oslund,
                                Dushyant Rao,
                                Allen Ren,
                                Baruch Tabanpour,
                                Quan Vuong,
                                Ayzaan Wahid,
                                Ted Xiao,
                                Ying Xu,
                                Vincent Zhuang,
                                Peng Xu†, Erik Frey†, Ken Caluwaerts†, Tingnan Zhang†, Brian Ichter†, Jonathan Tompson†, Leila Takayama†, Vincent Vanhoucke†, Izhak Shafran†, Maja Mataric†, Dorsa Sadigh†, Nicolas Heess†, Kanishka Rao†, Nik Stewart†, Jie Tan†, Carolina Parada†
                              </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><b>Google DeepMind</b></span>
                            
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <span class="icon">
                                    <a href="mailto:liangq@google.com,xiafei@google.com,magicmelon@google.com,andyzeng@google.com"><i class="fa fa-envelope"></i></a>
                                </span> corresponding authors in alphabetical order, †advising leads<br>all other authors in alphabetical order
                            </span>
                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a target="_blank" href="https://arxiv.org/abs/2402.11450"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a target="_blank" href="assets/lmpc.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>PDF</span>
                                    </a>
                                    <a target="_blank" href="https://colab.research.google.com/drive/1YcRN_kklw3cVVJNvgK_IEV6nDce9EJWK?usp=sharing"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                    <!-- <a target="_blank" href="??"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa-solid fa-desktop"></i>
                                        </span>
                                        <span>Colab</span>
                                    </a> -->
                                    <a href="#demo"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Demo</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <video poster="" id="" autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}">
                    <source src="videos/teaser.mp4" type="video/mp4">
                </video>
                 
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -
                            enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform newtasks.
                            However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant 
                            for only as long as it fits within the context size of the LLM,and can be forgotten over longer interactions. 
                            In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their 
                            teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task 
                            successful). 
                            Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process 
                            (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous 
                            interactions can be viewed as training a transition dynamics model - that can be combined with classic robotics techniques such as 
                            model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), 
                            a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments - improving non-expert teaching success 
                            rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. 
                            Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot 
                            embodiments and APIs by 31.5%. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <video autoplay muted loop width="100%" style="border-radius: 10px" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                    <source src="videos/teaching_demo.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0; margin-top: 32px">
        <div class="container is-max-desktop">
            <h2 class="title is-3"><span class="dvima">Supplemental Video:</span></h2>
            <div class="columns is-centered has-text-centered">
                <video poster="assets/images/pre.png" id="myVideo" controls loop width="100%">
                    <source src="videos/main_compressed.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Language Model Predictive Control</span></h2>
                        <p style="font-size: 125%">
                            Given a dataset of users teaching robots new tasks with language (represented as text inputs and code outputs from
                            online in-context learning – left), LMPC- Rollouts is trained to predict subsequent inputs and outputs conditioned on
                            the current chat history (middle), and uses MPC (receding horizon control) for inference-time search to return the next
                            best action (with fewest expected corrections before success). LMPC-Skip is an alternate variant that is trained to
                            directly predict the last action (right). Both LMPC variants accelerate fast robot adaptation via in-context learning.
                        </p>
                        <br>
                        <img src="assets/images/lmpc-rollout-vs-skip-v1.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Experiments</span></h2>
                        <p style="font-size: 125%">
                            Our experiments evaluate how much the various proposed finetuning strategies (slow adaptation) improve online in-context
                            learning (fast adaptation) for humans interactively teaching robots via natural language feedback. Evaluations are
                            performed on 78 robot tasks, across 5 robot embodiments in simulation and 2 on real hardware. We specifically explore
                            the following questions:

                        </p>

                        <ul>
                            <li>How much does fine-tuning improve teachability, especially on test tasks?</li>
                            <li>How do LMPC-Rollouts and LMPC-Skip compare?</li>
                            <li>What are the benefits of Top-User Conditioning?</li>
                            <li>Does finetuning enable cross-embodiment generalization?</li>
                            <li>Can iterative finetuning further improve teachability?</li>
                          </ul>
                        <br>
                        <img src="assets/images/main-experiment-plots.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                        <p style="font-size: 125%">
                            Our fine-tuned LLMs with LMPC-Rollouts and LMPC-Skip improve the teachability of the base model (PaLM 2-S), and
                            outperforms a RAG baseline across all embodiments. LMPC-Skip overfits to train tasks (left), while LMPC-Rollouts
                            generalizes better (i.e., more teachable and responsive to feedback) on unseen test tasks (right) for multi-turn
                            sessions (with more than one chat turn).
                        </p>
                        <br>
                        <img src="assets/images/table1.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                        <p style="font-size: 125%">
                            Comparing base and finetuned models across all embodiments. Success: overall success rate on all tasks. Num Chat Turns:
                            mean number of chat turns for successful chat sessions. Good Rating: proportion of positively rated chat turns after the
                            turn. Successful Tasks: proportion of tasks with at least one successful chat session. 1 turn Success: the proportion of
                            chat sessions that were successful with just one chat turn. 2+ turn Success: the proportion of chat sessions that were
                            successful with two or more chat turns. For both train and test tasks, LMPC-Skip achieves the lowest Num Chat Turns for
                            successful chat sessions, as well as the highest 1-turn Success Rate. These reflect how LMPC-Skip is trained to predict
                            the final code as fast as possible. However, LMPC-Rollouts has the highest 2+ turn Success Rate, suggesting it is most
                            amenable to corrective feedback given an incorrect first response. To maximize performance in practice, these results
                            suggest that one should use LMPC-Skip for responding to the initial user instruction, then LMPC-Rollouts for responding
                            to subsequent user feedback. For RAG, while the method improves upon the base model on overall success rate, it achieves
                            lower Successful Task Rate than the base model on test tasks. This suggests that while RAG may be proficient at
                            increasing the success rate of tasks similar to the retrieved examples, it struggles to perform well on novel tasks.
                        </p>
                        <br>
                        <img src="assets/images/table3.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <!-- <h2 class="title is-3"><span class="dvima">Teaching Demos</span></h2>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/teaching_demo.mp4" type="video/mp4">
                                  </video>
                                 
                            </div>
                            <p></p>
                        </div> -->

                        <p style="font-size: 125%">
                            We evaluate our approach on a subset of tasks for the Mobile Manipulator and the Robot Dog in the real world.
                            For each task, we ask users to perform four teaching sessions on the real-robot directly. See results that compare PaLM
                            2-S and LMPC-Rollouts in the table. LMPC-Rollouts achieves higher success rate than PaLM 2-S across all tasks. While Num
                            Chat Turns for successful sessions is about the same for PaLM 2-S and LMPC-Rollouts on these tasks, LMPC-Rollouts
                            achieves much higher success rates
                        </p>
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-widescreen" id="demo">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Teaching Demos</span></h2>
                        <p style="font-size: 125%">
                            Below we show some complex robot behaviors across many robot embodiments that can be taught using our system.</p>
                        <br>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/sim.mp4" type="video/mp4">
                                  </video>
                                 
                            </div>
                            
                        </div>

                        <br>
                        <p style="font-size: 125%">
                            Below we show our method can be applied to real robots, and there is a significant difference in robot behavior before and after teaching.</p>
                        <br>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/real.mp4" type="video/mp4">
                                  </video>
                            </div>
                            <p style="font-size: 125%">
                                Below we show our a chat session rolled out in the real world, applying reward conditioned distillation to change robot motions.</p>
                            <br>
                              
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/chatturn.mp4" type="video/mp4">
                                </video>
                            </div>
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/chatturn2.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                       
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{liang2024learning,
      title={Learning to Learn Faster from Human Feedback with Language Model Predictive Control}, 
      author={Jacky Liang and Fei Xia and Wenhao Yu and Andy Zeng and Montserrat Gonzalez Arenas and Maria Attarian and Maria Bauza and Matthew Bennice and Alex Bewley and Adil Dostmohamed and Chuyuan Kelly Fu and Nimrod Gileadi and Marissa Giustina and Keerthana Gopalakrishnan and Leonard Hasenclever and Jan Humplik and Jasmine Hsu and Nikhil Joshi and Ben Jyenis and Chase Kew and Sean Kirmani and Tsang-Wei Edward Lee and Kuang-Huei Lee and Assaf Hurwitz Michaely and Joss Moore and Ken Oslund and Dushyant Rao and Allen Ren and Baruch Tabanpour and Quan Vuong and Ayzaan Wahid and Ted Xiao and Ying Xu and Vincent Zhuang and Peng Xu and Erik Frey and Ken Caluwaerts and Tingnan Zhang and Brian Ichter and Jonathan Tompson and Leila Takayama and Vincent Vanhoucke and Izhak Shafran and Maja Mataric and Dorsa Sadigh and Nicolas Heess and Kanishka Rao and Nik Stewart and Jie Tan and Carolina Parada},
      year={2024},
      eprint={2402.11450},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
</code></pre>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">Acknowledgments</h2>
       <p>
        We thank John Guilyard for his expert animations, and Giles Ruscoe for beautiful renderings. We thank Steven Bohez,
        Yuval Tassa, Tom Erez, Murilo Martins, Rugile Pevceviciute, David Rendleman, and Connor Schenck for their dedication to
        ensuring we had strong simulated environments. We thank Travis Armstrong, Noah Brown, Spencer Goodrich, Craig Hickman,
        Atil Iscen, Jerad Kirkland, Jason Powell, Stefano Saliceti, Ron Sloat, Sergey Yaroshenko, Eddie Yu, Grace Vesom, and
        Jake Varley for additional robot platform support and robot lab operations. Special thanks to Michael Ahn, Kendra Byrne,
        Aleksandra Faust, René Wagner, Yuheng Kuang, Yao Lu, Yansong Pang, and Zhuo Xu for supporting this project.
        We thank all the teachers who volunteered to collect the robot teaching data. We also thank the Google DeepMind
        Visualization and Human Interaction teams for their help with the development and support of the chat interface. We also
        want to thank the entire Google DeepMind Robotics team whose tireless efforts can be traced to additional support on
        this paper. This includes Administrative, Product, Programs, and Strategy teams whose contributions impact all of the
        team’s successes. We also want to thank our friends in Google DeepMind and Google Research for their guidance,
        inspirational research, and even direct contributions.
        </p>
    </div>
</section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a href="https://eureka-research.github.io/">Eureka</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

<script>
    document.addEventListener('DOMContentLoaded', (event) => {
      let options = {
        root: null, // using the viewport as the bounding box
        rootMargin: '0px',
        threshold: 0.5 // play when 50% of the video is visible
      };
  
      let observer = new IntersectionObserver((entries, observer) => { 
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.play();
          } else {
            entry.target.pause();
          }
        });
      }, options);
  
      let video = document.getElementById('videoInView');
      observer.observe(video);
    });
  </script>

</html>
